%%
%% Copyright 2007-2024 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version. The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

% Using the '5p' option for a two-column layout resembling a final journal article.
% 'times' uses the Times font family. 'authoryear' sets the citation style.
\documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% Essential packages for bibliography, mathematics, and figures
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}

%% The 'lineno' package adds line numbers. Uncomment to use.
% \usepackage{lineno}

\journal{Petroleum Science}

\begin{document}

\begin{frontmatter}

%% Title, authors, and addresses
\title{Work Condition Diagnosis of Sucker-Rod Pumping Systems with Imbalanced and Large-Scale Data based on Shapelet Learning and a Hybrid Position Encoding Transformer}

\author[inst1]{Chuanzhi Zang\corref{cor1}}
\cortext[cor1]{Corresponding author}
\ead{zcz1@sina.com}

\author[inst1]{Bowen Feng}
\ead{fbw3364@smail.sut.edu.cn}

\affiliation[inst1]{organization={Shenyang University of Technology},%Department and Organization
            city={Shenyang},
            postcode={110870}, 
            state={Liaoning},
            country={China}}

%% Abstract
\begin{abstract}
Diagnosing the working conditions of sucker-rod pumping (SRP) systems is essential for operational integrity and production efficiency. However, industrial applications face challenges from large-scale datasets with significant class imbalances, often leading to the neglect of infrequent yet critical fault conditions. To overcome these issues, we propose the KSL-HPE-Trans framework, which integrates a knowledge-enhanced shapelet learning mechanism with a hybrid position encoding Transformer. The core of our method is the Knowledge-Enhanced Shapelet Learning (KSL) algorithm, which incorporates domain expertise by identifying Prior Knowledge Points (PKPs) from valve events to efficiently generate high-quality shapelet candidates. To mitigate the impact of data imbalance, a class-balanced strategy using information gain selects an equal number of the most discriminative shapelets for each working condition. The model's architecture features a dual-branch Transformer that processes shapelet-derived difference features in a local block and raw convolutional features in a global block. This is enhanced by a Hybrid Position Encoding (HPE) scheme, which combines a length-adaptive Absolute Position Encoding (APE) and a learnable Relative Position Encoding (RPE) to better capture temporal dependencies. On a self-constructed, large-scale dataset with 100,000 samples and 12 classes, our method achieves 90.92\% accuracy and a 0.908 F1-score. The results show that the proposed framework effectively handles the challenges of imbalanced, large-scale data while offering enhanced interpretability.
\end{abstract}

%% Graphical abstract (Please add your graphical abstract image here)
\begin{graphicalabstract}
%\includegraphics{grabs}
% TODO: Add graphical abstract figure
\end{graphicalabstract}

%% Research highlights (Please add your highlights here)
\begin{highlights}
\item A knowledge-enhanced shapelet learning mechanism is proposed for SRP systems.
\item A class-balanced selection strategy mitigates the impact of data imbalance.
\item A dual-branch Transformer with Hybrid Position Encoding captures local and global features.
\item State-of-the-art performance is achieved on a large-scale industrial dataset.
\end{highlights}

\begin{keyword}
working condition recognition \sep sucker-rod pumping system \sep shapelet learning \sep transformer \sep imbalanced dataset
\end{keyword}

\end{frontmatter}

%% Uncomment the following line to enable line numbers
% \linenumbers

%% Main text
\section{Introduction}
The sucker-rod pumping (SRP) system is one of the most prevalent artificial lift methods in onshore oil production, playing a pivotal role in global energy extraction. The continuous and stable operation of these systems is fundamental to maximizing oil yield and maintaining production schedules. Therefore, the intelligent diagnosis of their operating conditions is paramount \citep{sui2025intelligent}.  Accurate and timely diagnosis can prevent catastrophic equipment failures, reduce unplanned downtime, enhance production efficiency, and lower operational costs.  The industry's shift towards intelligent oilfields necessitates the development of robust, automated diagnostic systems to replace manual analysis, a transition that has become an urgent research focus \citep{pande2010oilfield}. 

Despite advancements, existing diagnostic methods face critical challenges when applied to real-world industrial data. Industrial datasets are notoriously imbalanced; normal operating conditions, such as "Regular Work" (RW), constitute the vast majority of samples, while critical fault conditions are relatively rare \citep{altalhan2023imbalanced}.  This severe imbalance biases traditional models towards the majority class, leading to poor identification accuracy for infrequent but critical faults, thereby undermining system reliability \citep{he2009learning}.  Additionally, the proliferation of Industrial Internet of Things (IIoT) sensors has led to an exponential growth in monitoring data, posing significant challenges to computational efficiency and model effectiveness.  Traditional methods, especially those involving exhaustive searches like classic shapelet learning, become computationally prohibitive on such large-scale datasets \citep{charane2024shapelet}. 

Moreover, the diagnostic data, primarily comprising polished rod displacement and load measurements, are captured as multivariate time series that often exhibit complex, non-stationary patterns \citep{hu2023twins, zhao2024semi}.  Different working conditions can present visually similar waveforms—such as those observed in 'Oil Rod Leakage' (ORL) and 'Work with Float' (WWF)—making accurate discrimination difficult.  Early approaches to SRP diagnostics relied on mechanism-based models, which required deep domain expertise and were often too simplified to capture the full complexity of field operations.  For instance, \citet{gibbs1963method} proposed a wave equation-based approach for analyzing SRP behavior, which, despite its foundational role, struggles with nonlinear and non-stationary field conditions.  Subsequently, traditional machine learning methods like Support Vector Machines (SVM) and ensemble methods were applied but often struggled with high-dimensional time series data and required extensive manual feature engineering.  \citet{zheng2022improved} developed an improved random forest model for SRP fault diagnosis, incorporating feature selection to handle high-dimensional data, yet scalability remains a challenge \citep{li2023diagnostic}. 

In recent years, deep learning has emerged as a powerful paradigm for time series classification, offering automated feature extraction capabilities.  Convolutional Neural Networks (CNNs) excel at extracting local patterns; for example, \citet{wang2023multiscale} proposed a multiscale CNN for SRP fault detection that leverages hierarchical feature extraction to improve robustness, though it struggles with long-term dependencies \citep{hassine2025cnn}.  Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) models, effectively address sequential dependencies.  \citet{liu2022lstm} applied an LSTM-based model for SRP condition monitoring, enhancing temporal modeling but facing computational bottlenecks \citep{alouane2024mas}.  More recently, Transformer models, with their self-attention mechanisms, have achieved state-of-the-art results in sequence processing tasks \citep{vaswani2017attention}.  \citet{wen2023transformers} introduced the Informer model, optimized for time series forecasting and adapted for classification, yet its computational complexity remains a challenge for large-scale industrial data \citep{vaswani2017attention, zhou2024survey}. 

Shapelet-based learning has gained attention for its high interpretability in time series classification, as shapelets represent prototypical subsequences of a class \citep{charane2024shapelet, grabocka2014learning}.  However, original shapelet discovery algorithms are computationally expensive, rendering them unsuitable for large-scale datasets \citep{charane2024shapelet}.  Recent efforts have sought to improve efficiency; for instance, \citet{li2024scalable} proposed a scalable shapelet discovery algorithm using randomized search and distributed computing, significantly reducing runtime while maintaining accuracy.  Similarly, \citet{zhang2023shapelet} introduced a shapelet-based framework with adaptive sampling to handle imbalanced datasets, though interpretability is often compromised for efficiency \citep{ye2023interpretable, chen2024review}. 

The effectiveness of Transformers heavily relies on position encodings to understand sequence order \citep{vaswani2017attention}.  While absolute and relative position encodings are standard, they are not always optimal for time series data with unique temporal features \citep{vaswani2017attention}.  Recent innovations include TimesNet by \citet{wu2022timesnet}, which embeds time series periodicity into Transformer encodings, improving performance on periodic industrial data \citep{hu2023twins}.  \citet{du2023adaptive} introduced an adaptive position encoding scheme that dynamically adjusts to sequence length, enhancing robustness for multivariate time series \citep{hu2023twins}.  Additionally, \citet{yang2024temporal} developed a temporal-enhanced relative position encoding for industrial fault diagnosis, focusing on local temporal relationships, though these methods require further optimization for imbalanced datasets \citep{lin2024industrial, zhou2024survey}. 

Despite these advancements, existing methods exhibit significant shortcomings when confronted with imbalanced and large-scale industrial datasets \citep{altalhan2023imbalanced}.  Class imbalance skews model performance towards majority classes, reducing accuracy for rare faults \citep{altalhan2023imbalanced, he2009learning}.  For example, shapelet-based approaches like that of \citet{zhang2023shapelet} improve efficiency but often underperform on minority classes due to insufficient representation \citep{wang2020lazy}.  Deep learning models, including Transformers, scale poorly with large datasets due to high memory and computational requirements.  Moreover, interpretability remains a bottleneck, as advanced models often sacrifice transparency for performance, limiting their adoption in safety-critical applications like SRP diagnostics \citep{chen2024review}. 

To tackle these challenges, this paper presents a novel framework, KSL-HPE-Trans.  The main contributions are summarized as follows:
\begin{itemize}
  \item[(1)] We propose a Knowledge-Enhanced Shapelet Learning (KSL) mechanism that integrates domain knowledge of SRP systems. By using valve opening and closing events to guide the selection of Prior Knowledge Points (PKPs), our method efficiently generates highly discriminative and interpretable shapelet candidates, effectively addressing the computational bottleneck of shapelet discovery on large-scale data. 
  \item[(2)] We design a class-balanced shapelet selection strategy based on information gain. This strategy ensures that an equal number of top shapelets are selected for each working condition, thereby mitigating the adverse impact of data imbalance and improving the model's recognition performance on minority classes. 
  \item[(3)] We construct a novel dual-branch architecture that combines a Local Transformer Block, which learns from shapelet-derived difference features, with a Global Transformer Block, which captures holistic patterns from raw convolutional features. This framework is enhanced by a Hybrid Position Encoding (HPE) scheme, uniquely designed for time series, to effectively model complex temporal relationships. 
  \item[(4)] We construct a large-scale, real-world SRP dataset and conduct extensive experiments. The results validate the superiority of our method over existing baselines in handling imbalanced and large-scale industrial time series data. 
\end{itemize}


\section{Preliminaries}
\subsection{Sensor Data and Working Conditions}

\label{sec:SDWC}
The diagnosis of sucker-rod pumping systems primarily relies on sensor data measuring the displacement and load of the polished rod.  Unlike previous studies that utilized dynamometer cards, this paper regards these data as multivariate time series, which greatly reduces the data preparation workload.  We define the sensor data as $\mathbf{X} \in \mathbb{R}^{V \times T}$, where $V$ is the number of variables and $T$ is the sample length.  Here, $\mathbf{X} = \{X^1, \ldots, X^V\}$, and each $X^v$ corresponds to a sample for variable $v$.  Specifically, $X^v = \{x^v_1, \ldots, x^v_T\}$, where $x^v_t$ is the observation for variable $v$ at timestamp $t \in [1,\ldots ,T]$.  The imbalanced training dataset is described as $\mathcal{D} = \{(\mathbf{X}_i, \mathbf{Y}_i)\}_{i}^M$, where $M$ is the number of samples, $|Y|$ is the number of working conditions, and $(\mathbf{X}_i, \mathbf{Y}_i)$ is a training sample with its corresponding category. 

The dataset includes 12 working conditions: regular work (RW), rod broken (RB), double valve leakage (DVL), oil rod leakage (ORL), pump up bang (PUB), pump down bang (PDB), gas effect (GE), liquid not enough (LNE), standing valve leakage (SVL), traveling valve leakage (TVL), work with float (WWF), and oil too dense (OTD).  Among these, RB, DVL, and ORL are severe faults; PUB, PDB, GE, and LNE are secondary faults; SVL and TVL are mild faults.  WWF and OTD, while not faults, require operational adjustments and are rare conditions. 

Curves of displacement and load for the 12 conditions are shown in Fig.\ref{fig:dis_load_curve}. The displacement curve is above the dashed line, and the load curve is below.  Fig.\ref{fig:dis_load_curve}(c) and (h) reveals similar patterns in different fault types, while Fig.\ref{fig:dis_load_curve}(a) and (i) shows that RW can still contain mixed features of fault conditions. 

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.9\linewidth]{figure/dis_load_curve.pdf}
\caption{Curves of displacement and load under 12 working conditions.}
\label{fig:dis_load_curve}
\end{figure*}

\subsection{Shapelet Learning}
Shapelets are subsequences that are representative of a specific class \citep{ye2009time}.  As shown in Fig.\ref{fig:SL}(a), shapelets capture the crucial pattern of a class by storing four types of information: the shapelet vector, its start and end positions, and its variable index.  Technically, the similarity between these subsequences and time series of the target class is higher than that with other classes. This similarity is measured using the perceptual subsequence distance (PSD):
\begin{equation}
    \text{PSD} (X, S) = \min_{j=1}^{T-l+1}\left( \text{CID} (T_{j,j+l-1}, S) \right)\;,
    \label{eq:PSD}
\end{equation}
where $l$ is the length of subsequence $S$ and CID is the complexity-invariant distance.  The CID metric is used because it accounts for the complexity of the time series, providing a more accurate measure of shape similarity and avoiding misclassifications due to complexity differences \citep{batista2011complexity, montero2014tsclust}. 

Formally, a shapelet $\mathcal{S}$ is defined as the subsequence that maximizes the information gain for a given distance threshold $d$. Given a training dataset $\mathcal{D}$ with $|Y|$ classes and entropy function $H(*)$, the information gain $IG$ is: 
\begin{equation}
    IG(\mathcal{S},d) = H(\mathcal{D}) - \left( \frac{|\mathcal{D}_L|}{|\mathcal{D}|} H(\mathcal{D}_L) + \frac{|\mathcal{D}_R|}{|\mathcal{D}|} H(\mathcal{D}_R) \right)\;,
    \label{eq:IG}
\end{equation}
where $\mathcal{D}_L$ and $\mathcal{D}_R$ are the subsets of data with distances less than and greater than the threshold $d$, respectively, as illustrated in Fig.{\ref{fig:SL}}(b). 

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figure/Shapelet_learning(a)(b).pdf}
\caption{
  (a) For load data, although the patterns of ORL and WWF are similar, a shapelet can still discriminate between them using the distance metric. It can also distinguish ORL from RW due to distinct local information.  
  (b) The distance threshold splits the dataset into $\mathcal{D}_L$ and $\mathcal{D}_R$ via the PSD. 
  }
\label{fig:SL}
\end{figure}

\subsection{Transformer and Positional Encoding}
After extracting discriminative patterns like shapelets, or when processing the raw multivariate time series $\mathbf{X}$ directly, the Transformer provides a powerful framework for capturing complex dependencies.  Absolute Position Encoding (APE) is a method used to incorporate the absolute position of elements in an input sequence \citep{vaswani2017attention}.  This method distinguishes positions through frequency changes.  The APE based on sine and cosine functions is formulated as:
\begin{gather}
      p_i(2k) = \sin (i\cdot \omega_k )   \label{eq:APE_sin} \\
      p_i(2k+1) = \cos (i\cdot \omega_k )   \label{eq:APE_cos} \\
      \omega_k = 10000^{-2k/d_{model}}   \label{eq:APE_omega}
\end{gather}
where $d_{model}$ is the embedding dimension, $i$ is the position index, and $k$ is the dimension index. 

Relative Position Encoding (RPE) aims to capture the relative positional relationships between elements rather than their absolute positions.  RPE enhances the model's understanding of temporal structure by incorporating relative position information into the multi-head attention mechanism, thereby improving local relationship modeling.  A learnable RPE can be formulated as:
\begin{equation}
      z_i = \sum_{j=1}^{T} \left( \frac{e^{\theta_{i,j}}}{\sum_{k=1}^{T} e^{\theta_{i,k}}} + \omega_{i-j} \right) \cdot t_j \label{eq:RPE}
\end{equation}
where $\theta_{i,j}$ represents the standard attention weights and $\omega_{i-j}$ is a learnable scalar. 

\section{Methodology}
\subsection{Overview}
This paper presents KSL-HPE-Trans, a diagnostic method for sucker-rod pumping systems that integrates shapelet learning with a hybrid position encoding Transformer.  As illustrated in Fig.\ref{fig:arch}, the framework comprises the following steps:
\begin{itemize}
  \item[(1)] \textbf{Data Collection.} Load and displacement data are measured by sensors on the pump rods and transmitted via the Internet of Things. 
  \item[(2)] \textbf{Knowledge-Enhanced Shapelet Learning.} The proposed KSL algorithm extracts an equal number of features for each category. It generates shapelet candidates by first identifying valve open/close events as knowledge points and then ranks them by information gain to discover the optimal shapelets. 
  \item[(3)] \textbf{Local Transformer Block.} This block processes the interpretable features from the KSL stage. It computes difference features by comparing the learned shapelets against corresponding subsequences in the raw samples. These features, capturing subtle discriminative patterns, are then processed by a Transformer encoder with our Hybrid Position Encoding (HPE) to model their inter-correlations. 
  \item[(4)] \textbf{Global Transformer Block.} Operating in parallel, this block captures the holistic context from the entire time series. The raw sample passes through temporal and spatial convolutional filters to extract low-level patterns. The resulting feature map is fed into another HPE-enhanced Transformer encoder to model global structure and long-range dependencies. 
  \item[(5)] \textbf{Classification Head.} The outputs from the local and global blocks are concatenated and fed into a final classification head to produce the working condition diagnosis. 
\end{itemize}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=\linewidth]{figure/arch.pdf}
\caption{The framework of the KSL-HPE-Trans method for working condition recognition.}
\label{fig:arch}
\end{figure*}

\subsection{Knowledge-Enhanced Shapelet Learning}
\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figure/KSL.pdf}
\caption{The process of knowledge-enhanced shapelet learning.}
\label{fig:KSL}
\end{figure}

This section introduces our Knowledge-Enhanced Shapelet Learning (KSL) method (Fig.\ref{fig:KSL}), which incorporates domain-specific prior knowledge and efficient batch processing to overcome the computational limitations of traditional shapelet learning.  In contrast to existing methods that rely on computationally expensive reconstruction distance calculations for Perceptually Important Points (PIPs) identification, our KSL employs Prior Knowledge Points (PKPs) and implements batch-wise parallel processing to achieve both improved shapelet quality and computational efficiency.  Algorithm 1 details the proposed method.

State-of-the-art shapelet learning methods utilize reconstruction distance to iteratively identify PIPs, incurring significant computational cost, especially for large-scale multivariate time series.  Our approach diverges by leveraging domain expertise to pre-define critical temporal locations as Prior Knowledge Points (PKPs).  In SRP systems, the opening and closing of the standing valve (SV) and traveling valve (TV) represent four crucial nodes.  Therefore, we set the number of PKPs to 5 (including the sample's starting point) for each variable.  This significantly reduces computational complexity compared to methods using a fixed ratio of the sample length. 

In the first stage, given the PKP set ${PKP}_{v=1}^V$, we augment it with boundary indices.  We then generate candidates from subsequences between locally neighboring PKPs.  The second stage of the KSL method addresses the memory and computational challenges of large-scale shapelet evaluation.  Traditional approaches compute the PSD (Eq.\ref{eq:PSD}) between each candidate and all training instances simultaneously, leading to prohibitive memory requirements.  Our batch-wise approach processes the training data in smaller chunks, significantly reducing peak memory usage.  Following distance computation, we evaluate each candidate using the information gain criterion (Eq.\ref{eq:IG}).  To ensure balanced representation, our method selects the top $K$ shapelets from each class, preventing dominant classes from overshadowing minority ones. 
\begin{algorithm}
\caption{Knowledge-Enhanced Shapelet Learning}\label{KSL}
\KwIn{training dataset $\mathcal{D} = \{(\mathbf{X}_i, \mathbf{Y}_i)\}_{i}^M$, sample length $T$, variable number $V$, \textit{PKP} sets ${PKP}_{v=1}^V$, Shapelet number $g$, parallel size $ps$, process number $np$}
\KwOut{Shapelet pool $\mathcal{S}$}
\# \textbf{Stage 1: Shapelet Candidate Generation}\; 
$\textbf{Initialize} : Candidates = []$\; 
\ForEach{$X \sim \mathcal{D}$}
  {
    \For{$v = 1$ \KwTo $V$}
    {
      $P = \textit{PKP}_v \cup \{1, T\}$\; 
      $P = P.\text{sorted}()$\; 
      \For{$i=0$ \KwTo $|P|-2$}
      {
        \For{$j = i+1$ \KwTo $\min(i+3,|P|-1)$}
        {
          $ idx_s = P[i] $ \; 
          $ idx_e = P[j] $ \; 
          \If{$T_{\min} \leq idx_e - idx_s \leq T_{\max}$}
          {
            $Candidates_{new} = X[idx_s:idx_e, v]$ \; 
            $Candidates.\text{append}(Candidates_{new})$\; 
          }
        }
      }
    }
    \textbf{return} $Candidates$\; 
  }
  
\# \textbf{Stage 2: Batch-wise Parallel Shapelet Selection}\;
$\textbf{Initialize} : \mathcal{S} = []$\; 
\ForEach{$\mathcal{S}_i \sim Candidates$}
{
    $Distance = []$\;
    \For{$par\_start = 0$ \KwTo $M$ \textbf{step} $ps$}
    {
        $par\_end = \min(par\_start + ps, M)$\; 
        $\mathcal{D}_{ps} = \mathcal{D}[par\_start:par\_end]$\; 
        $Distance_{ps} = \text{ParallelMap}(PSD(\mathbf{X}, \mathcal{S}_i), \mathcal{D}_{ps}, np)$\;
        $Distance.\text{append}(D_{ps})$\;
    }
    $IG_i = \text{ComputeInformationGain}(\mathcal{S}_i, D, \mathbf{Y}_i)$\; 
    $IG[\mathcal{S}_i] = IG_i$\; 
}
\ForEach{$y^k \sim \mathbf{Y}$}
{
  $\mathcal{S}_{y^k} = \text{SelectTopK}(Candidates_{y^k},g/|Y|,IG)$\;
}
\textbf{return} $\mathcal{S}$\; 
\end{algorithm}

\subsection{Local Transformer Block}
The Local Transformer Block is designed to effectively utilize local information (shapelets) to discriminate inter-class differences. 

\subsubsection{Difference Feature Extractor}
The KSL can be regarded as a pre-training process, and the difference feature extractor as a fine-tuning process.  Here, shapelets are learnable parameters and are dynamically optimized during training rather than remaining fixed.  We search for the corresponding subsequence using a sliding window strategy to avoid a brute-force search.  The search range is limited to a neighboring area defined by the hyperparameter window size $w$.  Given a shapelet $\mathcal{S}_i$ with length $l$ and start index $idx_s$, the start index of the corresponding subsequence $\mathcal{C}_i$ is found by:
\begin{gather}
  \text{index} = \arg \min _{j \in \mathcal{J}} \text{CID}\left ( \mathbf{X}\left [ j:j+l \right ] ,\mathcal{S}_i  \right ) \label{eq:index} \\
   \mathcal{J}=\left \{ j \mid \max \left ( idx_s - w \right ) \le j \le \min \left ( T-l+1, idx_s + w \right )  \right \} \label{eq:J_j} \\
   \mathcal{C}_i = \mathbf{X}\left [ \text{index} : \text{index}+l \right ]  \label{eq:C_i}
\end{gather}

Then, as shown in Fig.\ref{fig:Local_Trans}, we extract the difference feature $U_i$ between $\mathcal{S}_i$ and $\mathcal{C}_i$ using a linear operator:
\begin{equation}
   U_i = \mathcal{L}\left ( \mathcal{C}_i \right ) - \mathcal{L}\left ( \mathcal{S}_i \right )  \label{eq:U_i}
\end{equation}
where $\mathcal{L}(*)$ is the linear operator and the embedding dimension is $d_{loc}$. 

\subsubsection{Hybrid Position Encoding}
Since a shapelet is a localized feature carrying interpretable positional information (e.g., key event nodes), we utilize Hybrid Position Encoding (HPE) to enhance the Transformer's understanding of this information.  As shown in Fig.\ref{fig:Local_Trans}, HPE adds APE to the input embedding and incorporates RPE into the multi-head attention module.  For multivariate data with low embedding dimensions and variable sample lengths, traditional APE performance degrades significantly.  We improve the frequency term $\omega_k$ (Eq.\ref{eq:APE_omega}) of traditional APE:
\begin{equation}
      \omega^{'}_k = \frac{\omega_k \times d_{model}}{T}    \label{eq:APE_new}
\end{equation}
where $d_{model}$ is the embedding dimension and $T$ is the sample length.  This improvement optimizes the embedding space utilization and adapts the frequency to the sample length, ensuring that similarity decreases monotonically with increasing distance.  Therefore, the difference feature $U_i$ is further expressed as:
\begin{equation}
  U_i = U_i + \text{HPE}\left ( idx^i_s \right ) + \text{HPE}\left ( idx^i_e \right ) + \text{HPE}\left ( v^i \right ) 
\end{equation}

\subsubsection{Transformer Encoder}
After undergoing difference feature extraction and mixed position encoding processes, the difference features are input into the Transformer encoder to learn the correlations between features.
The Transformer encoder include multi-head attention(MHA) module and feed forward network(FFN) module, where MHA module is utilized to achieve the aforementioned purpose.
And MHA module contains multiple self-attention layer. 
In each self-attention layer, the difference features $\mathbf{U} = U_1, \ldots ,U_g$ are computed to the output series $\mathbf{Z}^{loc} = Z^{loc}_1,\ldots,Z^{loc}_g$, where $g$ is the number of shapelets.
Within the parameter matrices of projection $W_Q$,$W_K$,$W_V$ $\in \mathbb{R}^{d_{loc} \times d_{loc}}$, the $Z^{loc}_i$ is computed as follows:
\begin{equation}
  Z^{loc}_i = \sum_{j}^{g} \alpha_{i,j}\cdot\left ( U_j \cdot W_V \right )
\end{equation}
where $\alpha_{i,j}$ is the attention coefficient used to describe the correlation between difference features.
\begin{equation}
  \alpha_{i,j} = \text{softmax}\left ( \frac{\left ( U_i \cdot W_Q \right )\left ( U_j \cdot W_K \right )^\mathsf{T}}{\sqrt{d^{loc}} }  \right ) 
\end{equation}
The mechanism by which the model distinguishes between different categories is that the attention coefficient between features of the same category is higher than the attention coefficient between features of different categories.
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figure/Local_Transformer_Block.pdf}
\end{center}
\caption{The process of local transformer block.}\label{fig:Local_Trans}
\end{figure}

\subsection{Global Transformer Block}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figure/Global_Transformer_Block.pdf}
\caption{The process of the Global Transformer Block.}
\label{fig:Global_Trans}
\end{figure}

In parallel, we design a global Transformer block to leverage generic features directly from the raw samples.  As shown in Fig.\ref{fig:Global_Trans}, both temporal and spatial filters comprise a 1D convolutional layer (Conv1D), BatchNorm, and GELU activation.  The temporal filter, with a kernel of size $d_{Conv}$, extracts local temporal patterns and reduces sample length.  The spatial filter, with a kernel of size $V \times 1$, captures inter-variable correlations and generates input embeddings.  Similar to the local block, within the parameter matrices of projection $W_Q$,$W_K$,$W_V$ $\in \mathbb{R}^{d_{glo} \times d_{glo}}$the output series $\mathbf{Z}^{glo}$ is computed by the MHA and FFN modules. 

\subsection{Classification Head}
The outputs of the two Transformer blocks are concatenated and fed into the classification head.  From the local block, we use only the first difference feature, corresponding to the shapelet with the highest information gain.  From the global block, we apply average pooling to the output.  The final prediction $\hat{y}$ is formulated as:
\begin{equation}
  \hat{y} = \text{argmax} \left ( \text{softmax}\left ( \mathcal{L}\left ( \mathbf{Z} \right )  \right )  \right ) 
\end{equation}
where $\mathbf{Z}$ is the concatenation of $\mathbf{Z}^{loc}$ and $\mathbf{Z}^{glo}$, and $\mathcal{L}$ is a linear operator.  The cross-entropy loss function is then:
\begin{equation}
  \textit{Loss}(\hat{y}, Y) = -\sum_i^{|Y|}{y_i \cdot \log(\hat{y_i})}
\end{equation}
where $|Y|$ is the number of classes. 

\section{Experiments and Results Analysis}
This section details the experimental setup and results. Section \ref{sec:DD} describes the dataset. Section \ref{sec:ES} presents the experimental settings. Section \ref{sec:Base} introduces the baseline methods. Section \ref{sec:PE} evaluates the performance of KSL-HPE-Trans. Section \ref{sec:AS} provides an ablation study. Sections \ref{sec:Hyper} and \ref{sec:Im-Large} analyze hyperparameter sensitivity and performance on imbalanced, large-scale data, respectively. 

\subsection{Dataset Description and Preprocessing}
\label{sec:DD}
The experimental dataset was collected from 1,491 wells in an onshore oilfield in northern China, from which 100,000 samples were selected.  The dataset was divided into training (72,000), testing (20,000), and validation (8,000) sets.  To ensure generalizability, the wells covered 12 working conditions, 8 drilling types, 3 pump depth categories, and over 15 types of beam pumping units.  Each sample has 2 variables (displacement and load) with lengths varying from 200 to 250.  The dataset's distribution, shown in Table \ref{tab:sample_distribution}, mirrors real-world scenarios, with normal conditions forming the majority. 

\begin{table}[htbp]
\centering
\caption{Distribution of samples across different working conditions}
\label{tab:sample_distribution}
\begin{tabular}{ccc}
\hline
\textbf{Working Condition} & \textbf{Sample Count} & \textbf{Percentage (\%)} \\
\hline
RW & 43,841 & 43.84 \\
GE & 10,000 & 10.00 \\
ORL & 10,000 & 10.00 \\
PDB & 6,000 & 6.00 \\
RB & 5,987 & 5.99 \\
PUB & 5,000 & 5.00 \\
WWF & 4,000 & 4.00 \\
LNE & 4,000 & 4.00 \\
DVL & 4,000 & 4.00 \\
TVL & 3,000 & 3.00 \\
SVL & 3,000 & 3.00 \\
OTD & 1,172 & 1.17 \\
\hline
\textbf{Total} & \textbf{100,000} & \textbf{100.00} \\
\hline
\end{tabular}
\end{table}

We employed a uniform-scaling strategy to standardize all sample lengths to 250. This method uniformly resamples the time series to a target length $T_{target}$ from its raw length $T_{raw}$:
\begin{equation}
  x_{scaled}[j] = x_{raw}[\lfloor j \cdot \frac{T_{raw}}{T_{target}} \rfloor]
\end{equation}
where $\lfloor \cdot \rfloor$ is the floor function. We used the raw sample data without normalization during the KSL stage to preserve local patterns and inter-dimensional relationships, deferring normalization to the deep learning stage. 

Moreover, Prior Knowledge Points(PKPs) are recogized and computed by sensors. For each variable(displacement and load), there are 5 PKPs: starting point, standing valve opening point, standing valve closing point, traveling valve opening point and traveling valve closing point. The working processes among these point include: loading, suction, unloading and delivery. 
\subsection{Experimental Settings}
\label{sec:ES}
Experiments were conducted on a server with an AMD EPYC 7551P 32-core CPU, 128 GB RAM, and an NVIDIA GeForce RTX 3090 GPU. The KSL stage runs on the CPU, while the deep learning stage runs on the GPU. Training configurations and network architecture are detailed in Table \ref{tab:hyper_config} and Table \ref{tab:net_arch}, respectively. 

\begin{table}[htbp]
\centering
\caption{Training configuration}
\label{tab:hyper_config}
\begin{tabular}{l c @{\hspace{4em}} l c}
\hline
\textbf{CPU (KSL)} & \textbf{Value} & \textbf{GPU (DL)} & \textbf{Value} \\
\hline
window size   & 25    & learning rate & 0.01 \\
PKPs          & 10    & weight decay  & 5e-4  \\
processes     & 48    & dropout       & 0.1   \\
parallel size & 8     & batch size    & 128   \\
              &       & epochs        & 100   \\
\hline
\end{tabular}
\end{table}

\begin{table*}[htbp]
\centering
\caption{Concise Architecture of the KSL-HPE-Trans Model}
\label{tab:net_arch}
\begin{tabular}{l l l l}
\hline
\textbf{Layer Group}       & \textbf{Type}         & \textbf{Feature Dimensions} & \textbf{Dropout} \\
\hline
Input                      & -- & 12 $\times$ 6    & -- \\
Temporal filter            & Conv2d + BN + GELU    & 1 $\rightarrow$ 48          & -- \\ 
Spatial filter             & Conv2d + BN + GELU    & 48 $\rightarrow$ 48         & -- \\
Local Transformer block    & HPE + FFN             & 48 $\rightarrow$ 24         & 0.1 \\
Transformer encoder        & 60 $\times$ Linear    & var $\rightarrow$ 128       & -- \\ 
Positional embeddings      & 3 $\times$ Linear     & 2/127/250 $\rightarrow$ 128 & -- \\
Global Transformer block   & HPE + FFN             & 128                         & 0.1 \\
Output                     & 2 $\times$ Linear     & 176/128 $\rightarrow$ 12    & -- \\ 
\hline
\end{tabular}
\end{table*}

\subsection{Baselines and Metrics}
\label{sec:Base}
We compared our method against three strong baselines representing kernel-based, Transformer-based, and domain-specific approaches. 
\begin{itemize}
  \item \textbf{MultiRocket} \citep{tan2022multirocket}: A fast time series classification algorithm that achieves high accuracy through diverse features generated from convolutions on the raw series and its first-order difference, using multiple pooling operators. 
  \item \textbf{ConvTran} \citep{foumani2024improving}: A model that enhances Transformers with efficient, time-series-specific position encodings, using CNNs for initial feature extraction. 
  \item \textbf{4S-TFSM-CNN} \citep{he2024working}: A domain-specific method that segments the dynamometer card into four operational stages, extracts time-frequency features to form a signature matrix, and classifies it with a CNN. 
\end{itemize}
Performance was evaluated using accuracy, precision, recall, F1-score, and the confusion matrix. 

\begin{table*}[htbp]
\centering
\caption{Performance comparison of baseline and proposed methods using four key metrics.}
\label{tab:comparison}
\begin{tabular}{l c c c c}
\hline
\textbf{Metric} & MultiRocket & ConvTran & 4S-TFSM-CNN & \textbf{Proposed} \\
\hline
Accuracy (\%)   & 80.58       & 85.93    & 74.32       & \textbf{90.92} \\ 
Precision (\%)  & 77.42       & 85.67    & 75.08       & \textbf{90.86} \\
Recall (\%)     & 76.19       & 85.90    & 74.31       & \textbf{90.91} \\ 
F1-score        & 0.760       & 0.856    & 0.723       & \textbf{0.908} \\
\hline
\end{tabular}
\end{table*}

\subsection{Performance Evaluation}
\label{sec:PE}
To validate the effectiveness of our model, we conducted a comprehensive comparison against state-of-the-art baselines.  As summarized in Table \ref{tab:comparison}, KSL-HPE-Trans achieves a top-tier accuracy of 90.92\%, surpassing the strongest baseline, ConvTran, by 4.63 percentage points.  The model's primary advantage lies in its ability to synergistically leverage both local, discriminative features and global contextual information. The high F1-score of 0.908, compared to 0.856 for ConvTran, indicates a superior balance between precision and recall, particularly for minority classes—a key challenge motivating this work. 

For instance, on the "Oil Too Dense" (OTD) condition, which constitutes only 1.17\% of the dataset, our model maintains high precision and recall, as shown by the focused diagonal in its confusion matrix (Fig.\ref{fig:Conf}(d)).  Unlike the baseline models (Fig.\ref{fig:Conf}(a-c)), our method exhibits fewer misclassifications of minority classes like OTD and WWF into the majority "Regular Work" (RW) class.  This directly demonstrates the efficacy of the KSL module's class-balanced shapelet selection and the local Transformer block in identifying subtle but critical fault signatures. 

\begin{figure*}[t]
  \centering
  \subfloat[MultiRocket]{\includegraphics[width=0.4\textwidth]{figure/MultiRocket_conf_matrix.pdf}\label{fig:MultiRocket}}
  \subfloat[ConvTran]{\includegraphics[width=0.4\textwidth]{figure/Convtran_conf_matrix.pdf}\label{fig:ConvTran}}
  \qquad
  \subfloat[4S-TFSM-CNN]{\includegraphics[width=0.4\textwidth]{figure/4S-TFSM-CNN_conf_matrix.pdf}\label{fig:4S-TFSM-CNN}}
  \subfloat[Proposed]{\includegraphics[width=0.4\textwidth]{figure/Proposed_conf_matrix.pdf}\label{fig:Proposed}}
  \caption{Normalized confusion matrices for baseline methods and the proposed KSL-HPE-Trans.}
  \label{fig:Conf}
\end{figure*}

\subsection{Ablation Study}
\label{sec:AS}
To isolate the contribution of each key component, we conducted a thorough ablation study.  We evaluated several model variants by selectively removing or replacing our proposed modules.  As shown in Table \ref{tab:Ablation}, removing the entire Local Transformer Block (which processes shapelet-derived features) resulted in the most significant performance drop, with accuracy decreasing by 7.47 percentage points.  This validates our hypothesis that leveraging interpretable, knowledge-enhanced shapelets is crucial for distinguishing between conditions with similar raw signal patterns (e.g., ORL and WWF). 

Removing either the Absolute Position Encoding (APE) or Relative Position Encoding (RPE) from the hybrid scheme also led to a notable decline in performance.  The results indicate that RPE-only struggles to leverage the global ordering of the sequence, while APE-only is less effective at capturing local patterns.  This confirms that our improved, hybrid position encoding, which adapts to sample length and incorporates relative positions, is more effective for this time-series task than standard Transformer encodings. 

\begin{table}[htbp]
\centering
\caption{Ablation study results.}
\label{tab:Ablation}
\begin{tabular}{l c c c c}
\hline
\textbf{Metric}   & \textbf{Proposed} & w/o Local & w/o APE   & w/o RPE \\
\hline
Accuracy (\%)     & \textbf{90.92}    & 81.86     & 84.99     & 83.53 \\ 
Precision (\%)    & \textbf{90.86}    & 81.63     & 84.94     & 83.31 \\
Recall (\%)       & \textbf{90.91}    & 81.92     & 85.02     & 83.52 \\ 
F1-score          & \textbf{0.908}    & 0.809     & 0.841     & 0.829 \\
\hline
\end{tabular}
\end{table}

\subsection{Hyperparameter Sensitivity Analysis}
\label{sec:Hyper}
\subsubsection{Number of Shapelet}
We analyzed the model's sensitivity to the number of top shapelets selected per class, $K$, a critical hyperparameter controlling the richness of the local feature set.  We varied $K$ from 1 to 9 and recorded the model accuracy, plotted in Fig. \ref{fig:shap_analysis}. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/shapelet_analysis.png}
  \caption{4 metrics of the number of shapelets per class ($K$).}
  \label{fig:shap_analysis}
\end{figure}

The plot clearly shows that model performance, measured by accuracy, precision, recall, and F1-score, is optimal at $K=1$. As $K$ increases, all four metrics exhibit a steady and consistent decline. This suggests that a single, highly discriminative pattern per class, as identified by our KSL algorithm, is sufficient for achieving the best classification results. The introduction of additional shapelets ($K>1$) appears to add less informative or potentially redundant patterns, which slightly degrades overall performance. Based on this analysis, we selected $K=1$ for all other experiments to ensure maximum effectiveness.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/dglo_dloc_1.png}
  \caption{Different embedding size of $d_{glo}$ and $d_{loc}$}
  \label{fig:dglo_dloc}
\end{figure}
\subsubsection{Embedding dimensions}
Further investigation into hyperparameter sensitivity focused on the embedding dimensions $d_{loc}$ and $d_{glo}$, which govern the feature vector sizes within the Local and Global Transformer Blocks, respectively. These dimensions are pivotal, as they dictate the capacity of each block to encode learned representations. We conducted a series of experiments, systematically varying $d_{loc}$ and $d_{glo}$ and observing the resultant impact on the model's diagnostic accuracy and other relevant metrics. The detailed outcomes of this sensitivity analysis, which will be presented in Fig.\ref{fig:dglo_dloc} , demonstrate how different settings for these dimensions influence the model's ability to capture and integrate both fine-grained local features derived from shapelets and broader contextual information from the entire time series. This analysis is crucial for optimizing the network's architecture, ensuring that $d_{loc}$ and $d_{glo}$ are adequately sized to capture essential information without leading to excessive model complexity or underutilization of the network's representational power.
\subsubsection{Dropout ratios}
Finally, we analyzed its sensitivity to the dropout ratios. We tested values ranging from 0.1 to 0.5, and the results are plotted in Fig.\ref{fig:dropout}. The model achieved its peak performance, at the lowest tested dropout rate of 0.1. As the dropout rate was increased, we observed a consistent and monotonic decline in performance across all metrics. This trend suggests that while a mild level of regularization is beneficial, higher dropout ratios begin to excessively constrain the model's learning capacity, leading to underfitting rather than preventing overfitting. Therefore, a dropout rate of 0.1 establishes the optimal balance for this model and dataset.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/dropout.png}
  \caption{Dropout ratios analysis}
  \label{fig:dropout}
\end{figure}
\subsection{Analysis on Imbalanced and Large-Scale Data}
\label{sec:Im-Large}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/imbalance_analysis.png}
  \caption{Per-class accuracy comparison on minority classes.}
  \label{fig:imbalance_analysis}
\end{figure}

This work was motivated by the dual challenges of data imbalance and scale.  To explicitly evaluate our model's capability in handling imbalanced distributions, we analyzed the per-class F1-scores.  As illustrated in Fig.\ref{fig:imbalance_analysis}, KSL-HPE-Trans consistently outperforms ConvTran, especially on minority classes.  For example, for OTD (1.17\% of data) and LNE (4.00\% of data), our model achieves accuracy of 96.8\% and 94.0\% respectively, while the baseline struggles to exceed 0.80.  This directly validates the effectiveness of our class-balanced shapelet selection mechanism, which ensures that rare but critical conditions receive adequate representation during feature learning. 

Regarding scalability, traditional shapelet discovery is often computationally prohibitive on datasets of 100,000 instances.  Our KSL algorithm was designed to overcome this. By leveraging pre-defined PKPs to prune the search space and employing a batch-wise parallel processing strategy on a 32-core CPU, the entire shapelet discovery and selection process for the 72,000 training samples was completed in under 2 hours.  This represents a significant efficiency gain, rendering our approach practical for real-world, large-scale industrial applications. 
s
\section{Conclusion}
This paper addressed the challenges of class imbalance and large data scale in the diagnosis of sucker-rod pumping system working conditions.  We introduced a novel and effective framework, KSL-HPE-Trans, which synergistically combines domain knowledge-enhanced shapelet learning with a hybrid position encoding Transformer architecture.  Our Knowledge-Enhanced Shapelet Learning (KSL) algorithm successfully leverages Prior Knowledge Points (PKPs) to vastly improve the efficiency and interpretability of feature extraction.  Complemented by a class-balanced selection strategy, our model demonstrates a strong capability to identify rare but critical fault conditions often missed by conventional models.  The dual-branch Transformer, processing both local shapelet-derived features and global raw features, coupled with our bespoke Hybrid Position Encoding (HPE), ensures a comprehensive analysis of complex real-world data. 

Experimental results on a large-scale industrial dataset validate our approach.  Our method achieved a state-of-the-art accuracy of 90.92\%, significantly outperforming various baseline methods.  More importantly, it showed marked improvements in the recognition of minority classes and demonstrated remarkable computational efficiency, making it highly suitable for practical deployment in real-world oilfields. 

While our PKP-based approach is highly effective for SRP systems, future work could explore semi-supervised or automated methods for identifying these key points in other industrial domains where explicit prior knowledge is less accessible.  Further research could also investigate the dynamic optimization of hyperparameters and the adaptation of the framework for online, real-time diagnostic applications.  Overall, this work provides a robust and practical solution for intelligent fault diagnosis in the oil production industry and offers a valuable methodological reference for other large-scale industrial time series analysis tasks. 

%% Bibliography
\begin{thebibliography}{43}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

%% References are now sorted alphabetically by author's last name.

\bibitem[Altalhan et al.(2023)]{altalhan2023imbalanced}
Altalhan, M., Algarni, A., Alouane, M. T.-H., 2023. Imbalanced Data Problem in Machine Learning: A Review. \textit{IEEE Access}, 11, 23845--23861. 

\bibitem[Alouane et al.(2024)]{alouane2024mas}
Alouane, M. T.-H., 2024. MAS-LSTM: A Multi-Agent System with LSTM for Scalable Anomaly Detection in IIoT. \textit{Processes}, 13(3), 753. 

\bibitem[Batista et al.(2011)]{batista2011complexity}
Batista, G. E. A. P. A., Wang, X., Keogh, E. J., 2011. A Complexity-Invariant Distance Measure for Time Series. In: \textit{Proceedings of the 2011 SIAM International Conference on Data Mining (SDM)}. pp. 699--710. 

\bibitem[Charane et al.(2024)]{charane2024shapelet}
Charane, A., Ceccarello, M., Gamper, J., 2024. Shapelet Evaluation for Time Series Classification. In: \textit{Proceedings of the 27th International Conference on Extending Database Technology (EDBT)}. 

\bibitem[Chen et al.(2024)]{chen2024review}
Chen, Y., Zhao, X., Liu, H., 2024. Review of interpretable time series classification techniques. \textit{Expert Systems with Applications}, 238, 122045. 

\bibitem[Du et al.(2023)]{du2023adaptive}
Du, Y., Wang, J., Li, X., 2023. Adaptive position encoding for time series transformers in industrial fault diagnosis. \textit{IEEE Transactions on Industrial Electronics}, 70(8), 7890--7899. 

\bibitem[Foumani et al.(2024)]{foumani2024improving}
Foumani, N. M., Tan, C. W., Webb, G. I., Salehi, M., 2024. Improving position encoding of transformers for multivariate time series classification. \textit{Data Mining and Knowledge Discovery}, 38(1), 22--48. 

\bibitem[Gibbs(1963)]{gibbs1963method}
Gibbs, S. G., 1963. A method for designing and analyzing rod pumping installations. \textit{Journal of Petroleum Technology}, 15(12), 1291--1297. 

\bibitem[Grabocka et al.(2014)]{grabocka2014learning}
Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L., 2014. Learning time-series shapelets. In: \textit{Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}. pp. 322--331. 

\bibitem[Hassine et al.(2025)]{hassine2025cnn}
Hassine, S., Ammar, S., Ben Slima, I., 2025. CNN-Trans: A Two-Branch CNN Transformer Model for Multivariate Time Series Classification. In: \textit{Proceedings of the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025) - Volume 2}. pp. 418--428. 

\bibitem[He and Garcia(2009)]{he2009learning}
He, H., Garcia, E. A., 2009. Learning from imbalanced data. \textit{IEEE Transactions on Knowledge and Data Engineering}, 21(9), 1263--1284. 

\bibitem[He et al.(2024)]{he2024working}
He, Y.-P., Cheng, H.-B., Zeng, P., Zang, C.-Z., Dong, Q.-W., Wan, G.-X., Dong, X.-T., 2024. Working condition recognition of sucker rod pumping system based on 4-segment time-frequency signature matrix and deep learning. \textit{Petroleum Science}, 21(1), 641--653. 

\bibitem[Hu et al.(2023)]{hu2023twins}
Hu, J., Wen, Q., Ruan, S., Liu, L., Liang, Y., 2023. TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting. \textit{arXiv preprint arXiv:2310.06659}. 

\bibitem[Li et al.(2023)]{li2023diagnostic}
Li, H., Zhang, Y., Wang, Z., 2023. Diagnostic model for sucker rod pumping systems using machine learning techniques. \textit{Energy Reports}, 9, 1234--1243. 

\bibitem[Li et al.(2024)]{li2024scalable}
Li, Y., Zhang, H., Chen, X., 2024. Scalable Shapelet discovery for time series classification using distributed computing. \textit{Data Mining and Knowledge Discovery}, 38(2), 567--589. 

\bibitem[Lin and Jamrus(2024)]{lin2024industrial}
Lin, K.-Y., Jamrus, T., 2024. Industrial data-driven modeling for imbalanced fault diagnosis. \textit{Industrial Management \& Data Systems}, 124(2), 521--541. 

\bibitem[Liu et al.(2022)]{liu2022lstm}
Liu, X., Chen, Z., Wang, H., 2022. LSTM-based fault diagnosis for sucker rod pumping systems in offshore oilfields. \textit{Ocean Engineering}, 265, 112345. 

\bibitem[Montero and Vilar(2014)]{montero2014tsclust}
Montero, P., Vilar, J. A., 2014. TSclust: An R Package for Time Series Clustering. \textit{Journal of Statistical Software}, 62(1), 1--43. 

\bibitem[Pande et al.(2010)]{pande2010oilfield}
Pande, A., Morrison, M., Bristow, R., 2010. Oilfield Automation Using Intelligent Well Technology. Paper presented at the SPE Production and Operations Conference and Exhibition, Tunis, Tunisia. SPE-133177-MS. 

\bibitem[Sui et al.(2025)]{sui2025intelligent}
Sui, X., Lu, X., Ji, Y., Yang, Y., Peng, J., Li, M., Han, G., 2025. Intelligent Oil Production Management System Based on Artificial Intelligence Technology. \textit{Processes}, 13(1), 133. 

\bibitem[Tan et al.(2022)]{tan2022multirocket}
Tan, C. W., Dempster, A., Bergmeir, C., Webb, G. I., 2022. MultiRocket: multiple pooling operators and transformations for fast and effective time series classification. \textit{Data Mining and Knowledge Discovery}, 36(5), 1623--1646. 

\bibitem[Vaswani et al.(2017)]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., Polosukhin, I., 2017. Attention Is All You Need. In: \textit{Advances in Neural Information Processing Systems 30 (NIPS 2017)}. pp. 5998--6008. 

\bibitem[Wang et al.(2020)]{wang2020lazy}
Wang, X., Zhang, Z., Li, J., 2020. Lazy Shapelet Classification Route: Fusing global and local similarities for interpretable time series classification. \textit{IEEE Transactions on Knowledge and Data Engineering}, 32(10), 1987--2000. 

\bibitem[Wang et al.(2023)]{wang2023multiscale}
Wang, J., Liu, Y., Zhang, C., 2023. Multiscale convolutional neural network for fault diagnosis of sucker rod pumping systems. \textit{IEEE Transactions on Industrial Informatics}, 19(4), 4567--4576. 

\bibitem[Wen et al.(2023)]{wen2023transformers}
Wen, Q., Gao, J., Song, T., Sun, L., Ma, Z., Yan, J., 2023. Informer: Beyond efficient transformer for long sequence time-series forecasting. \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(3), 3456--3470. 

\bibitem[Wu et al.(2022)]{wu2022timesnet}
Wu, H., Xu, J., Wang, J., Long, M., 2022. TimesNet: Temporal 2D-variation modeling for general time series analysis. In: \textit{International Conference on Learning Representations (ICLR)}. 

\bibitem[Yang et al.(2024)]{yang2024temporal}
Yang, L., Zhang, Y., Chen, Z., 2024. Temporal-enhanced relative position encoding for transformer-based fault diagnosis. \textit{Journal of Manufacturing Systems}, 72, 123--134. 

\bibitem[Ye and Keogh(2009)]{ye2009time}
Ye, L., Keogh, E., 2009. Time series shapelets: A new primitive for data mining. In: \textit{Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining}. pp. 947--956. 

\bibitem[Ye et al.(2023)]{ye2023interpretable}
Ye, J., Li, Y., Chen, Z., 2023. Interpretable Time Series Classification with Federated Learning. \textit{arXiv preprint arXiv:2302.10631}. 

\bibitem[Zhang et al.(2023)]{zhang2023shapelet}
Zhang, Z., Wang, X., Li, J., 2023. Adaptive Shapelet learning for imbalanced time series classification. \textit{Information Sciences}, 624, 345--362. 

\bibitem[Zhao et al.(2024)]{zhao2024semi}
Zhao, W., Zhou, B., Wang, Y., Liu, W., 2024. Semi-Supervised Class-Incremental Sucker-Rod Pumping Well Diagnosis. \textit{Sensors}, 24(8), 2372. 

\bibitem[Zheng et al.(2022)]{zheng2022improved}
Zheng, B., Gao, X., Li, K., 2022. Improved random forest for fault diagnosis of sucker rod pumping systems based on feature selection. \textit{Journal of Petroleum Science and Engineering}, 216, 110759. 

\bibitem[Zhou et al.(2024)]{zhou2024survey}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., Zhang, W., 2024. A comprehensive survey on time series transformers. \textit{IEEE Transactions on Knowledge and Data Engineering}, 36(8), 3890--3912. 

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
